## 爬虫代码部署：

### 将爬虫代码部署到其他服务器(此案例中为Ubuntu服务器)步骤：

1. 首先服务器要已经安装了scrapyd

   `pip3 install scrapyd`

2. 从`/usr/local/lib/python3.5/dist-packages/scrapyd`下拷贝出`default_scrapyd.conf`放到`/etc/scrapyd/scrapyd.conf`

3. 将`/etc/scrapyd/scrapyd.conf`中的bind_address修改为服务器的IP地址

4. 服务器这里最好重新安装一下twisted:

   ```
   pip uninstall twistedq
   pip install twisted==18.9.0
   ```

   因为默认安装的twisted包版本过高

5. 在本机scrapy项目中的`scrapy.cfg`文件中的`url`里的服务器ip地址替换为服务器的IP地址（有时候端口号也需要改变，视情况而定）

6. 在本机cmd中进入到项目里，输入命令`pip install scrapyd-client`安装客户端的scrapyd

   ==自此服务器与本机部署前的准备完毕==

7. 在服务器下的`/srv`文件夹下创建一个新的文件夹（这里创建为`lj`）,进入到 `lj`文件夹中，输入命令`scrapyd`开启服务器的scrapyd服务

   ==这里在服务器便可以看到开启后对应的ip地址和端口号，此时无论通过本机还是服务器都可以访问到这个“url”==

8. 在本机里cmd中的项目下，输入命令`scrapyd-deploy`，如果显示为不可执行命令，CSDN，一搜就有

   ==显示成功后，自此成功将本机爬虫代码部署到服务器==

### 运行与停止部署到服务器上的爬虫：

- 运行爬虫：curl http://192.168.205.128:6800/schedule.json -d project=lj -d spider=house

  其中的`url`为你服务器的IP地址和端口号，`project`为项目名

+ 关闭爬虫： curl http://192.168.205.128:6800/cancel.json -d project=lj -d job=6487ec79947edab326d6db28a2d86511e8247444

  其中的`url`为你服务器的IP地址和端口号，`project`为项目名，`job`为你运行爬虫时后得到的jobid

+ 更过API的命令：https://scrapyd.readthedocs.io/en/stable/api.html

### 如果想要部署到多台服务器：

+ 如果想要把爬虫代码布置到多台服务器

  + 保证每一台服务器都像一台服务器进行部署并成功开启scrapyd服务

  + 本机中的项目里的`scrapy.cfg`文件里要配置好，并且给每个deploy起一个名字，类似这样：

    ```python
    [deploy:lj1]
    url = http://192.168.205.128:6800/
    project = lj
    
    [deploy:lj2]
    url = http://192.168.205.130:6800/
    project = lj
    ```

  + 本机中的cmd中进入项目里后输入命令`scrapyd-deploy -a`

    意思是部署到所有已经开启且存在`scrapy.cfg`文件里的服务器

## 分布式爬虫运行：

### 概念：

​			分布式爬虫运行时，无论是本机还是部署了爬虫的服务器，都属于爬虫服务器，即拥有爬虫代码，负责爬取数据的，除此之外还需要有redis服务器，类似于分布式爬虫项目中的管理者

### 原理图解：



### 步骤：

1. 修改爬虫代码

   + 将爬虫的类从`scrapy.Spider`变成`scrapy_redis.spiders.RedisSpider`；或者是从`scrapy.CrawlSpide`r变成 `scrapy_redis.spiders.RedisCrawlSpider`

   + 将爬虫中的`start_urls`删掉。增加一个`redis_key="xxx"`。这个`redis_key`是为了以后在`redis`中控制爬虫启动的。 从redis的实现原理可以看出，爬虫服务器爬取数据不是各自爬各自的，而是听从redis的命令

   + 修改`settings.py`配置文件:

     + `SCHEDULER = "scrapy_redis.scheduler.Scheduler"`：修改SCHEDULER，因为新的`schduler`不再需要原来的一些比如`url`去重等功能

     + `DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"`：确保所有爬虫都共享相同的去重指纹

     + ```python
       ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300
       }
       ```

       设置新的ITEM_PIPELINES，从原理可知原来的pipelines已经不再负责保存，所以要把原文件中的ITEM_PIPELINES注释掉

     + `SCHEDULER_PERSIST = True`：数据持久化的标志位，在redis中保持scrapy-redis用到的队列，不会清理redis中的队列，从而可以实现暂停和恢复功能

     + ```
       REDIS_HOST = '192.168.175.129'
       REDIS_PORT = 6379
       ```

       设置连接redis信息

2. Ubuntu上安装了`Redis`，并且修改了`/etc/redis/redis.conf`配置文件，将bind中增加自己的IP地址

3. 将代码部署到其他的爬虫服务器上，并运行

   ==此时会进入等待redis服务器发出爬取命令的状态==

4. 开启redis服务器的redis，并在redis服务中通过命令`redis-cli -h 127.0.0.1 -p 6379`进入到redis服务中

5. 运行爬虫（redis服务器发号施令）

   + 在Redis服务器上推入一个开始的url链接`lpush[redis_key] start_url`便开始爬取
   + 爬取后的数据会保存在redis服务器中
   + 此时在redis服务器通过命令`keys *`可以看到3个键值，分别是调度器、请求、item，前两个负责request的调度，item负责数据保存
   + 更多redis命令：http://redisdoc.com/index.html



