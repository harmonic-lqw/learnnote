# 各论文提供的预训练模型：

## pSp

+ 人脸
+ （虽然没提供其他域，但是可以自己训练）

## e4e

+ 人脸
+ 车
+ 马
+ 教堂

## restyle

+ 人脸：FFHQ70000——CelebA_HQ2824
+ 车：Stanford Cars8144——1000
+ 马：LSUN_Horse10000——2215
+ 教堂：LSUN_Church126227——300
+ 动物：AFHQ wild4739——500
+ net里有这样一段代码：self.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))，在生成图像后会自适应的平均池化到（256，256），也就是说对于wild数据集，是先生成512*512的图像（因为生成器会生成这么大的图像），但是返回之前，会经过这样的一段代码将其调整到（256，256），因为训练集和测试集的数据在训练时都已经调整到（256，256），因此这样才能正常计算损失
+ inference的时候，如果不设置resize_outputs，推理出的图片将会是默认生成器最大（人脸：1024，动物：512），但如果打开了该选项，则车域将会生成(256, 192)大小，其他域将会生成(256, 256)大小，这与训练和测试阶段保持了一致
+ 但是无论inference时得到的results图像有多大，在计算L2和LPIPS损失的时候，都会先变成(256, 256)再进行计算，这从代码中便可以看到

## E2style

+ 人脸
+ （论文做了猫、马、车的实验，但是没有提供预训练模型）

## HFGI

+ 

# 记录

+ pSp有很大伪影
+ 尝试用Restyle，
  + pSp or e4e? 观察定性和定量指标，以为e4e因为判别损失的加入，使重建的定量指标不如pSp，但是编辑性较强
  + 训练马（训练数据和测试数据都是自己随机选定），所以可以观察有没有con的区别在哪
  + 如果效果好还可以借鉴E2style修改map2style模块（用平均池化替代卷积）试试

# 编辑

+ E2style有编辑的执行代码
+ SAM有车的大部分方向
+ HypterInvert和HypterStyle有church和car的编辑
+ HypterInvert对church_psp成功进行编辑



# Experiments

+ 比较实验-psp、e4e、restyle_psp、restyle_e4e

+ church做反演、编辑
+ car做反演、编辑！——目前看在车上的效果不好
+ horse做反演
+ wild做反演
+ 在人脸上验证效果
+ 做从后往前哪几个阶段（5，543，全阶段）使用con的消融实验

