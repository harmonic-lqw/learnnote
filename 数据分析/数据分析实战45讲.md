# 预习篇

## 数据分析的三个组成：

1. 数据采集。是我们的原材料，任何分析都要有数据源
2. 数据挖掘。进行数据分析的目的就是找到其中的规律，来指导我们的业务，因此数据挖掘的核心是挖掘数据的商业价值，也就是我们所谈的商业智能BI
3. 数据可视化。是数据领域中万金油的技能，可以让我们直观地了解数据分析的结果

![数据分析三个组成](D:\note_file\数据分析\数据分析三个组成.jpg)

+ 从思维到工具再到实践
+ **人与人最大的差别在于认知。只有把知识转化为自己的语言，才能真正变成我们自己的东西。这个转换的过程就是认知的过程**

## 数据挖掘的基本流程：

1. 商业理解：要从商业的角度理解项目需求，在这个基础上对数据挖掘的目标进行定义
2. 数据理解：收集数据，对数据进行探索，包括数据描述、数据质量验证、初步认识数据
3. 数据准备：收集数据，并进行数据清洗、数据集成操作、完成数据挖掘前的准备工作
4. 模型建立：选择和应用各种数据挖掘模型，并进行优化，比便得到更好的分类结果
5. 模型评估：对模型进行评价，检查构建模型的步骤，确认模型是否实现预定的目标
6. 上线发布：可以以一份报告的形式呈现、也可以是一个比较复杂、可重复的数据挖掘过程

## 数据挖掘的十大算法：

### 分类算法：

+ C4.5（决策树的算法）——创造性地 在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理。
+ 朴素贝叶斯——朴素贝叶斯模型是基于概率论的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。
+ SVM（支持向量机）——SVM 在训练 中建立了一个超平面的分类模型。
+ KNN（K最近邻算法）——所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。如果一个样本，它的 K 个最接近的邻居都属于分类 A， 那么这个样本也属于分类 A。
+ Adaboost——Adaboost 在训练中建立了一个联合的分类模型。boost 在英文中代表提升的意思，所以 Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类 器，所以 Adaboost 也是一个常用的分类算法。
+ CART（分类和回归树）——它 构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。

### 聚类算法：

+ K-Means——把物体划分为K类，每个 类别里面，都有个“中心点”，即意见领袖，它是这个类别的核心。现在我有一个新点要归 类，这时候就只要计算这个新点与 K 个中心点的距离，距离哪个中心点近，就变成了哪个 类别。
+ EM（最大期望算法）——原理是这样的：假设我 们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以 得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。

### 关联分析：

+ Apriori——Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集 （frequent item sets）来揭示物品之间的关联关系。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存 在很强的关系。

### 连接分析：

+ PageRank——PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇 论文的影响力越强。同样 PageRank 被 Google 创造性地应用到了网页权重的计算中：当 一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越 高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

# python工具

**深浅拷贝：**区别于浅拷贝只拷贝顶层引用，深拷贝会逐层进行拷贝，直到拷贝的所有引用都是不可变引用为止。

![浅拷贝](D:\note_file\数据分析\浅拷贝.jpg)

![深拷贝](D:\note_file\数据分析\深拷贝.jpg)

## Numpy库：

### 普通数组：

```python
import numpy as np
a = np.array([1, 2, 3])
b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
b[1,1]=10
print a.shape
print b.shape
print a.dtype
print b
```

### 结构数组：

```python
import numpy as np
persontype = np.dtype({
    'names':['name', 'age', 'chinese', 'math', 'english'],
    'formats':['S32','i', 'i', 'i', 'f']})
peoples = np.array([("ZhangFei",32,75,100, 90),("GuanYu",24,85,96,88.5),
       ("ZhaoYun",28,85,92,96.5),("HuangZhong",29,65,85,100)],
    dtype=persontype)
ages = peoples[:]['age']
chineses = peoples[:]['chinese']
maths = peoples[:]['math']
englishs = peoples[:]['english']
print np.mean(ages)
print np.mean(chineses)
print np.mean(maths)
print np.mean(englishs)
```

### 连续数组：

`np.arange(1,11,2)` 参数：初始值、终值（不包括）、步长

`np.linspace(1,9,5)`参数：初始值、终值（包括）、元素个数

### 加减乘除、求n次方、求余

```python
x1 = np.arange(1,11,2)
x2 = np.linspace(1,9,5)
print np.add(x1, x2)
print np.subtract(x1, x2)
print np.multiply(x1, x2)
print np.divide(x1, x2)
print np.power(x1, x2)
print np.remainder(x1, x2).

[ 2.  6. 10. 14. 18.]
[0. 0. 0. 0. 0.]
[ 1.  9. 25. 49. 81.]
[1. 1. 1. 1. 1.]
[1.00000000e+00 2.70000000e+01 3.12500000e+03 8.23543000e+05
 3.87420489e+08]
[0. 0. 0. 0. 0.]
```

### 最大最小值

```python
import numpy as np
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.amin(a)  # 整个矩阵
print np.amin(a,0)  # 按照列向量来看
print np.amin(a,1)  # 按照行向量来看
print np.amax(a)
print np.amax(a,0)
print np.amax(a,1)

1
[1 2 3]
[1 4 7]
9
[7 8 9]
[3 6 9]
```

### 最大与最小值之差

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.ptp(a)
print np.ptp(a,0)
print np.ptp(a,1)

8
[6 6 6]
[2 2 2]
```

### 百分位数

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.percentile(a, 50)   # 第P哥百分位数，p取从0到100，p=0求最小值、p=50求平均值、p=100求最大值
print np.percentile(a, 50, axis=0)   
print np.percentile(a, 50, axis=1)

5.0
[4. 5. 6.]
[2. 5. 8.]
```

### 中位数、平均数

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
# 求中位数
print np.median(a)
print np.median(a, axis=0)
print np.median(a, axis=1)
# 求平均数
print np.mean(a)
print np.mean(a, axis=0)
print np.mean(a, axis=1)

5.0
[4. 5. 6.]
[2. 5. 8.]
5.0
[4. 5. 6.]
[2. 5. 8.]
```

### 加权平均值

```python
a = np.array([1,2,3,4])
wts = np.array([1,2,3,4])
print np.average(a)
print np.average(a,weights=wts)

2.5
3.0
```

### 标准差std()、方差var()

```python
a = np.array([1,2,3,4])
print np.std(a)  # 方差是每个数与平均值的差，求平方，再求和，再求平均值
print np.var(a)  # 标准差是方差的算术平方根。在数学意义上，代表的是一组数据离平均值的分散程度

1.118033988749895
1.25
```

### 排序

```python
a = np.array([[4,3,2],[2,4,1]])
# sort(a, axis=-1, kind=‘quicksort’, order=None) 
# axis默认为-1，表示按照数组的最后一个轴进行排序，kind可以指定quicksort、mergesort、heapsort分别表示快速排序、归并排序、堆排序，默认为快速排序，order字段表示对于结构化的数组可以指定按照某个字段进行排序
print np.sort(a)  
print np.sort(a, axis=None)
print np.sort(a, axis=0)
print np.sort(a, axis=1)

[[2 3 4]
 [1 2 4]]
[1 2 2 3 4 4]
[[2 3 1]
 [4 4 2]]
[[2 3 4]
 [1 2 4]]
```

## Pandas库：

### 两个核心数据结构：

+ Series：是个定长的字典序列，在存储的时候相当于两个ndarray。有两个基本属性——index和values

  ```python
  import pandas as pd
  from pandas import Series, DataFrame
  x1 = Series([1,2,3,4])
  x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])
  d = {'a':1, 'b':2, 'c':3, 'd':4}
  x3 = Series(d)
  print x1
  print x2
  print x3 
  
  0    1
  1    2
  2    3
  3    4
  dtype: int64
  a    1
  b    2
  c    3
  d    4
  dtype: int64
  a    1
  b    2
  c    3
  d    4
  dtype: int64
  ```

+ DataFrame：包括行索引和列索引

  ```python
  import pandas as pd
  from pandas import Series, DataFrame
  # DataFrame第一个参数是字典型的数据，表示列方向上的数据，第二个参数可以定义行索引，默认为自然数，其数量和数据中的行数一致，第三个参数指定列的顺序，默认为原始顺序
  data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}
  df1= DataFrame(data)
  df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'], columns=['English', 'Math', 'Chinese'])  
  print df2
  
              English  Math  Chinese
  ZhangFei         65    30       66
  GuanYu           85    98       95
  ZhaoYun          92    96       93
  HuangZhong       88    77       90
  DianWei          90    90       80
  ```

### 数据的导入和输出：

+ Pandas 允许直接从 xlsx，csv 等文件中导入数据，也可以输出到 xlsx, csv 等文件

  ```python
  import pandas as pd
  from pandas import Series, DataFrame
  score = DataFrame(pd.read_excel('data.xlsx'))
  score.to_excel('data1.xlsx')
  print score
  ```

### 数据清洗：

+ 删除不必要的行和列：`DataFrame.drop(index=['行'])`和`DataFrame.drop(columns=['列'])`

+ 重命名列名：`DataFrame.rename(columns={'原列名':'新列名'}, inplace = True)`
+ 去掉重复行：`DataFrame.drop_duplicates()`
+ 更改数据格式：`DataFrame['列名'].astype(类型)`
+ 删除空格/左边空格/右边空格：`DataFrame['列名']=DataFrame['列名'].map(str.strip/str.lstrip/str.rstrip)`
+ 删除特殊符号：`DataFrame['列名']=DataFrame['列名'].str.strip('符号')`
+ 大小写转换：
  + 全部大写：`df2.columns = df2.columns.str.upper()`
  + 全部小写：`df2.columns = df2.columns.str.lower()`
  + 首字母大写：`df2.columns = df2.columns.str.title()`
+ 查找空值：
  + 什么地方有空值：`DataFrame.isnull()`
  + 哪一列有空值：`DataFrame.isnull().any()`

+ 使用apply：

  + 对name列进行大写转化：`df['name'] = df['name'].apply(str.upper)`

  + 定义函数进行处理：

    ```python
    def double_df(x):
        return 2*x
    df1[u'语文'] = df1[u'语文'].apply(double_df)  # 将语文成绩乘2处理（这里的u表示后面字符串以 Unicode 格式 进行编码，一般用在中文字符串前面，防止因为源码储存格式问题，导致再次使用时出现乱码）
    
    def plus(df,n,m):
        df['new1'] = (df[u'语文']+df[u'英语']) * m
        df['new2'] = (df[u'语文']+df[u'英语']) * n
        return df
    df1 = df1.apply(plus,axis=1,args=(2,3,))  # 新增两列，第一个参数是函数名，axis=1代表按照列为轴进行操作、axis=0代表行为轴进行操作，args传递参数，最终生成新的df
    ```

### 数据统计：

![pandas数据统计](D:\note_file\数据分析\pandas数据统计.jpg)

### 数据表合并(merge())：

+ 基于指定的列进行连接：`df3 = pd.merge(df1, df2, on='name')`——合并的是name列相同的索引
+ inner内连接：`df3 = pd.merge(df1, df2, how='inner')`——其实就是键的交集，合并的是所有相同的键的相同的索引
+ outer外连接：`df3 = pd.merge(df1, df2, how='outer')`——相当于求两个DataFrame的并集
+ left左连接：`df3 = pd.merge(df1, df2, how='left')`——以第一个 DataFrame 为主进行的连接，第二个 DataFrame 作为补充（必须有一列是相同的）
+ right右连接：`df3 = pd.merge(df1, df2, how='right')`——以第二个 DataFrame 为主进行的连接，第一个 DataFrame 作为补充（必须有一列是相同的）

### 使用SQL方式打开Pandas：

```python
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)
```

# 数据可视化

## 目的：

+ 分布、时间相关、局部/整体、偏差、相关性、排名、量级、地图、流动

## 常用工具：

+ 八爪鱼数据抓取、微图数据可视化
+ Tableau：商业数据可视化软件
+ DataV：数字大屏技术
+ Python语言可视化
+ R语言可视化

## Python数据可视化简述：

### 常用视图：

![常用视图](D:\note_file\数据分析\常用视图.jpg)

### 视图呈现的关系：

1. 比较：比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；
2. 联系：查看两个或两个以上变量之间的关系，比如散点图；
3. 构成：每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；
4. 分布：关注单个变量，或者多个变量的分布情况，比如直方图。

### 散点图：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
N = 1000
# numpy.random.randn(d0,d1,…,dn)是从标准正态分布中返回一个或多个样本值
x = np.random.randn(N)
y = np.random.randn(N)
# 用 Matplotlib 画散点图
plt.scatter(x, y, marker='x')
plt.show()
# 用 Seaborn 画散点图
df = pd.DataFrame({'x': x, 'y': y})
# sns.jointplot(x, y, data=None, kind=‘scatter’) 函数，data一般是DataFrame数据，前两个参数分别是DataFrame中的两个列的列名作为横纵坐标
sns.jointplot(x="x", y="y", data=df, kind='scatter');
plt.show()
```

### 折线图：

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]
# 使用 Matplotlib 画折线图
# 在 Matplotlib 中，我们可以直接使用 plt.plot() 函数，当然需要提前把数据按照 x 轴的大小进行排序，要不画出来的折线图就无法按照 x 轴递增的顺序展示
plt.plot(x, y)
plt.show()
# 使用 Seaborn 画折线图
# 在 Seaborn 中，我们使用 sns.lineplot (x, y, data=None) 函数。其中 x、y 是 data 中的下标。data 就是我们要传入的数据，一般是 DataFrame 类型
df = pd.DataFrame({'x': x, 'y': y})
sns.lineplot(x="x", y="y", data=df)
plt.show()
```

### 直方图：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
a = np.random.randn(100)
s = pd.Series(a) 
# 用 Matplotlib 画直方图
# 在 Matplotlib 中，我们使用 plt.hist(x, bins=10) 函数，其中参数 x 是一维数组，bins 代表直方图中的箱子数量，默认是 10
plt.hist(s)
plt.show()
# 用 Seaborn 画直方图
# 在 Seaborn 中，我们使用 sns.distplot(x, bins=10, kde=True) 函数。其中参数 x 是一维数组，bins 代表直方图中的箱子数量，kde 代表显示核密度估计
sns.distplot(s, kde=False)
plt.show()
sns.distplot(s, kde=True)
plt.show()
```

### 条形图：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = ['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5']
y = [5, 4, 8, 12, 7]
# 用 Matplotlib 画条形图
# 在 Matplotlib 中，我们使用 plt.bar(x, height) 函数，其中参数 x 代表 x 轴的位置序列，height 是 y 轴的数值序列，也就是柱子的高度
plt.bar(x, y)
plt.show()
# 用 Seaborn 画条形图
# 在 Seaborn 中，我们使用 sns.barplot(x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量
sns.barplot(x, y)
plt.show()
```

### 箱线图：

```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
# 数据准备
# 生成 0-1 之间的 10*4 维度数据
data=np.random.normal(size=(10,4))
print(data)
lables = ['A','B','C','D']
# 用 Matplotlib 画箱线图
# 在 Matplotlib 中，我们使用 plt.boxplot(x, labels=None) 函数，其中参数 x 代表要绘制箱线图的数据，labels 是缺省值，可以为箱线图添加标签
plt.boxplot(data,labels=lables)
plt.show()
# 用 Seaborn 画箱线图
# 在 Seaborn 中，我们使用 sns.boxplot(x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量
df = pd.DataFrame(data, columns=lables)
print(df)
sns.boxplot(data=df)
plt.show()
```

### 饼图：

```python
import matplotlib.pyplot as plt
# 数据准备
nums = [25, 37, 33, 37, 6]
labels = ['High-school','Bachelor','Master','Ph.d', 'Others']
# 用 Matplotlib 画饼图
# 在 Matplotlib 中，我们使用 plt.pie(x, labels=None) 函数，其中参数 x 代表要绘制饼图的数据，labels 是缺省值，可以为饼图添加标签
plt.pie(x = nums, labels=labels)
plt.show()
```

### 热力图：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
flights = sns.load_dataset("flights")
data=flights.pivot('year','month','passengers')
# 用 Seaborn 画热力图
sns.heatmap(data)
plt.show()
```

### 蜘蛛图：

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.font_manager import FontProperties  
# 数据准备
labels=np.array([u" 推进 ","KDA",u" 生存 ",u" 团战 ",u" 发育 ",u" 输出 "])
stats=[83, 61, 95, 67, 76, 88]
# 画图数据准备，角度、状态值
angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
stats=np.concatenate((stats,[stats[0]]))
angles=np.concatenate((angles,[angles[0]]))
# 用 Matplotlib 画蜘蛛图
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)   
ax.plot(angles, stats, 'o-', linewidth=2)
ax.fill(angles, stats, alpha=0.25)
# 设置中文字体
font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf", size=14)  
ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)
plt.show()
```

### 二元变量分布：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
tips = sns.load_dataset("tips")
print(tips.head(10))
# 用 Seaborn 画二元变量分布图（散点图，核密度图，Hexbin 图）
# 在 Seaborn 里，使用二元变量分布是非常方便的，直接使用 sns.jointplot(x, y, data=None, kind) 函数即可。其中用 kind 表示不同的视图类型：“kind=‘scatter’”代表散点图，“kind=‘kde’”代表核密度图，“kind=‘hex’ ”代表 Hexbin 图，它代表的是直方图的二维模拟
sns.jointplot(x="total_bill", y="tip", data=tips, kind='scatter')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='kde')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='hex')
plt.show()
```

### 成对关系：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
iris = sns.load_dataset('iris')
# 用 Seaborn 画成对关系
# 探索数据集中的多个成对双变量的分布，可以直接采用 sns.pairplot() 函数。它会同时展示出 DataFrame 中每对变量的关系，另外在对角线上，你能看到每个变量自身作为单变量的分布情况
sns.pairplot(iris)
plt.show()
```

# 数据分析中的基本概念

+ 数据挖掘核心：包括分类、聚类、预测、关联分析

## 元数据和数据源：

+ 元数据：描述其它数据的数据，也称为“中介数据”
+ 数据元：最小的数据单元

## 数据挖掘中的任务：

### 分类：

+ 通过训练集得到一个分类模型，然后用这个模型可以对其他数据（测试集）进行分类。

### 聚类：

+ 将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。

### 预测：

+ 通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。

### 关联分析：

+ 发现数据中的关联规则

## 数据挖掘的步骤：

1. 输入数据
2. 数据预处理
   1. 数据清洗：为了去除重复数据，去噪声以及填充缺失值
   2. 数据集成：将多个数据源中的数据存放在一个统一的数据存储中
   3. 数据变换：就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 0~1 之间。
3. 数据挖掘
4. 后处理：将模型预测的结果进一步处理后再导出
5. 信息

+ **上帝不会告诉我们规律，而是展示给我们数据**

## 数据清洗：

### 规则：

+ **完**整性：单条数据是否存在空值，统计的字段是否完善。

+ **全**面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。

+ **合**法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。

+ 唯**一**性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

## 数据集成：

### 两个架构：ETL和ELT

![image-20210810125019181](D:\note_file\数据分析\ETL和ELT.png)

### ETL的典型工具：Kettle

+ 两个脚本
  + Transformation（转换）：分成三个步骤，它包括了输入、中间转换以及输出。
    + Step（步骤）：Step 是转换的最小单元，每一个 Step 完成一个特定的功能。
    + Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。
  + Job（作业）：完整的任务，实际上是将创建好的转换和作业串联起来
    + Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务。
    + Hop（跳跃线）：连接 Job Entry 的线。并且它可以指定是否有条件地执行。

## 数据变换（比算法选择更重要）：

### 数据平滑：

+ 去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑

### 数据聚集：

+ 对数据进行汇总

### 数据概化：

+ 将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念

### 数据规范化：

+ 使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等

+ **最大—最小规范化**：将原始数据变换到 [0,1] 的空间中。用公式表示就是：`新数值 =（原数值 - 极小值）/（极大值 - 极小值）`

  ```python
  from sklearn import preprocessing
  import numpy as np
  # 初始化数据，每一行表示一个样本，每一列表示一个特征
  x = np.array([[ 0., -3.,  1.],
                [ 3.,  1.,  2.],
                [ 0.,  1., -1.]])
  # 将数据进行 [0,1] 规范化
  min_max_scaler = preprocessing.MinMaxScaler()
  minmax_x = min_max_scaler.fit_transform(x)
  print minmax_x
  ```

+ **Z-Score规范化**：`新数值 =（原数值 - 均值）/ 标准差`，结果易于比较但是没有实际意义

  ```python
  from sklearn import preprocessing
  import numpy as np
  # 初始化数据
  x = np.array([[ 0., -3.,  1.],
                [ 3.,  1.,  2.],
                [ 0.,  1., -1.]])
  # 将数据进行 Z-Score 规范化
  scaled_x = preprocessing.scale(x)
  # Z-Score 规范化将数据集进行了规范化，数值都符合均值为 0，方差为 1 的正态分布。
  print scaled_x
  ```

+ **小数定标规范化**：通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。`比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。`

  ```python
  from sklearn import preprocessing
  import numpy as np
  # 初始化数据
  x = np.array([[ 0., -3.,  1.],
                [ 3.,  1.,  2.],
                [ 0.,  1., -1.]])
  # 小数定标规范化
  j = np.ceil(np.log10(np.max(abs(x))))
  scaled_x = x/(10**j)
  print scaled_x
  ```

+ python工具：SciKit-Learn库

### 属性构造：

+ 构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程



































































































