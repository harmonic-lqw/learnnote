# 预习篇

## 数据分析的三个组成：

1. 数据采集。是我们的原材料，任何分析都要有数据源
2. 数据挖掘。进行数据分析的目的就是找到其中的规律，来指导我们的业务，因此数据挖掘的核心是挖掘数据的商业价值，也就是我们所谈的商业智能BI
3. 数据可视化。是数据领域中万金油的技能，可以让我们直观地了解数据分析的结果

![数据分析三个组成](D:\note_file\数据分析\数据分析三个组成.jpg)

+ 从思维到工具再到实践
+ **人与人最大的差别在于认知。只有把知识转化为自己的语言，才能真正变成我们自己的东西。这个转换的过程就是认知的过程**

## 数据挖掘的基本流程：

1. 商业理解：要从商业的角度理解项目需求，在这个基础上对数据挖掘的目标进行定义
2. 数据理解：收集数据，对数据进行探索，包括数据描述、数据质量验证、初步认识数据
3. 数据准备：收集数据，并进行数据清洗、数据集成操作、完成数据挖掘前的准备工作
4. 模型建立：选择和应用各种数据挖掘模型，并进行优化，比便得到更好的分类结果
5. 模型评估：对模型进行评价，检查构建模型的步骤，确认模型是否实现预定的目标
6. 上线发布：可以以一份报告的形式呈现、也可以是一个比较复杂、可重复的数据挖掘过程

## 数据挖掘的十大算法：

### 分类算法：

+ C4.5（决策树的算法）——创造性地 在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理。
+ 朴素贝叶斯——朴素贝叶斯模型是基于概率论的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。
+ SVM（支持向量机）——SVM 在训练 中建立了一个超平面的分类模型。
+ KNN（K最近邻算法）——所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。如果一个样本，它的 K 个最接近的邻居都属于分类 A， 那么这个样本也属于分类 A。
+ Adaboost——Adaboost 在训练中建立了一个联合的分类模型。boost 在英文中代表提升的意思，所以 Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类 器，所以 Adaboost 也是一个常用的分类算法。
+ CART（分类和回归树）——它 构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。

### 聚类算法：

+ K-Means——把物体划分为K类，每个类别里面，都有个“中心点”，即意见领袖，它是这个类别的核心。现在我有一个新点要归类，这时候就只要计算这个新点与 K 个中心点的距离，距离哪个中心点近，就变成了哪个类别。
+ EM（最大期望算法）——原理是这样的：假设我 们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以 得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。

### 关联分析：

+ Apriori——Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集 （frequent item sets）来揭示物品之间的关联关系。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存 在很强的关系。

### 连接分析：

+ PageRank——PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇 论文的影响力越强。同样 PageRank 被 Google 创造性地应用到了网页权重的计算中：当 一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越 高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

# python工具

**深浅拷贝：**区别于浅拷贝只拷贝顶层引用，深拷贝会逐层进行拷贝，直到拷贝的所有引用都是不可变引用为止。

![浅拷贝](D:\note_file\数据分析\浅拷贝.jpg)

![深拷贝](D:\note_file\数据分析\深拷贝.jpg)

## Numpy库：

### 普通数组：

```python
import numpy as np
a = np.array([1, 2, 3])
b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
b[1,1]=10
print a.shape
print b.shape
print a.dtype
print b
```

### 结构数组：

```python
import numpy as np
persontype = np.dtype({
    'names':['name', 'age', 'chinese', 'math', 'english'],
    'formats':['S32','i', 'i', 'i', 'f']})
peoples = np.array([("ZhangFei",32,75,100, 90),("GuanYu",24,85,96,88.5),
       ("ZhaoYun",28,85,92,96.5),("HuangZhong",29,65,85,100)],
    dtype=persontype)
ages = peoples[:]['age']
chineses = peoples[:]['chinese']
maths = peoples[:]['math']
englishs = peoples[:]['english']
print np.mean(ages)
print np.mean(chineses)
print np.mean(maths)
print np.mean(englishs)
```

### 连续数组：

`np.arange(1,11,2)` 参数：初始值、终值（不包括）、步长

`np.linspace(1,9,5)`参数：初始值、终值（包括）、元素个数

### 加减乘除、求n次方、求余

```python
x1 = np.arange(1,11,2)
x2 = np.linspace(1,9,5)
print np.add(x1, x2)
print np.subtract(x1, x2)
print np.multiply(x1, x2)
print np.divide(x1, x2)
print np.power(x1, x2)
print np.remainder(x1, x2).

[ 2.  6. 10. 14. 18.]
[0. 0. 0. 0. 0.]
[ 1.  9. 25. 49. 81.]
[1. 1. 1. 1. 1.]
[1.00000000e+00 2.70000000e+01 3.12500000e+03 8.23543000e+05
 3.87420489e+08]
[0. 0. 0. 0. 0.]
```

### 最大最小值

```python
import numpy as np
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.amin(a)  # 整个矩阵
print np.amin(a,0)  # 按照列向量来看
print np.amin(a,1)  # 按照行向量来看
print np.amax(a)
print np.amax(a,0)
print np.amax(a,1)

1
[1 2 3]
[1 4 7]
9
[7 8 9]
[3 6 9]
```

### 最大与最小值之差

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.ptp(a)
print np.ptp(a,0)
print np.ptp(a,1)

8
[6 6 6]
[2 2 2]
```

### 百分位数

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.percentile(a, 50)   # 第P哥百分位数，p取从0到100，p=0求最小值、p=50求平均值、p=100求最大值
print np.percentile(a, 50, axis=0)   
print np.percentile(a, 50, axis=1)

5.0
[4. 5. 6.]
[2. 5. 8.]
```

### 中位数、平均数

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
# 求中位数
print np.median(a)
print np.median(a, axis=0)
print np.median(a, axis=1)
# 求平均数
print np.mean(a)
print np.mean(a, axis=0)
print np.mean(a, axis=1)

5.0
[4. 5. 6.]
[2. 5. 8.]
5.0
[4. 5. 6.]
[2. 5. 8.]
```

### 加权平均值

```python
a = np.array([1,2,3,4])
wts = np.array([1,2,3,4])
print np.average(a)
print np.average(a,weights=wts)

2.5
3.0
```

### 标准差std()、方差var()

```python
a = np.array([1,2,3,4])
print np.std(a)  # 方差是每个数与平均值的差，求平方，再求和，再求平均值
print np.var(a)  # 标准差是方差的算术平方根。在数学意义上，代表的是一组数据离平均值的分散程度

1.118033988749895
1.25
```

### 排序

```python
a = np.array([[4,3,2],[2,4,1]])
# sort(a, axis=-1, kind=‘quicksort’, order=None) 
# axis默认为-1，表示按照数组的最后一个轴进行排序，kind可以指定quicksort、mergesort、heapsort分别表示快速排序、归并排序、堆排序，默认为快速排序，order字段表示对于结构化的数组可以指定按照某个字段进行排序
print np.sort(a)  
print np.sort(a, axis=None)
print np.sort(a, axis=0)
print np.sort(a, axis=1)

[[2 3 4]
 [1 2 4]]
[1 2 2 3 4 4]
[[2 3 1]
 [4 4 2]]
[[2 3 4]
 [1 2 4]]
```

## Pandas库：

### 两个核心数据结构：

+ Series：是个定长的字典序列，在存储的时候相当于两个ndarray。有两个基本属性——index和values

  ```python
  import pandas as pd
  from pandas import Series, DataFrame
  x1 = Series([1,2,3,4])
  x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])
  d = {'a':1, 'b':2, 'c':3, 'd':4}
  x3 = Series(d)
  print x1
  print x2
  print x3
  
  0    1
  1    2
  2    3
  3    4
  dtype: int64
  a    1
  b    2
  c    3
  d    4
  dtype: int64
  a    1
  b    2
  c    3
  d    4
  dtype: int64
  ```

+ DataFrame：包括行索引和列索引

  ```python
  import pandas as pd
  from pandas import Series, DataFrame
  # DataFrame第一个参数是字典型的数据，表示列方向上的数据，第二个参数可以定义行索引，默认为自然数，其数量和数据中的行数一致，第三个参数指定列的顺序，默认为原始顺序
  data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}
  df1= DataFrame(data)
  df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'], columns=['English', 'Math', 'Chinese'])  
  print df2
  
              English  Math  Chinese
  ZhangFei         65    30       66
  GuanYu           85    98       95 
  ZhaoYun          92    96       93
  HuangZhong       88    77       90
  DianWei          90    90       80
  ```

### 数据的导入和输出：

+ Pandas 允许直接从 xlsx，csv 等文件中导入数据，也可以输出到 xlsx, csv 等文件

  ```python
  import pandas as pd
  from pandas import Series, DataFrame
  score = DataFrame(pd.read_excel('data.xlsx'))
  score.to_excel('data1.xlsx')
  print score
  ```

### 数据清洗：

+ 删除不必要的行和列：`DataFrame.drop(index=['行'])`和`DataFrame.drop(columns=['列'])`

+ 重命名列名：`DataFrame.rename(columns={'原列名':'新列名'}, inplace = True)`
+ 去掉重复行：`DataFrame.drop_duplicates()`
+ 更改数据格式：`DataFrame['列名'].astype(类型)`
+ 删除空格/左边空格/右边空格：`DataFrame['列名']=DataFrame['列名'].map(str.strip/str.lstrip/str.rstrip)`
+ 删除特殊符号：`DataFrame['列名']=DataFrame['列名'].str.strip('符号')`
+ 大小写转换：
  + 全部大写：`df2.columns = df2.columns.str.upper()`
  + 全部小写：`df2.columns = df2.columns.str.lower()`
  + 首字母大写：`df2.columns = df2.columns.str.title()`
+ 查找空值：
  + 什么地方有空值：`DataFrame.isnull()`
  + 哪一列有空值：`DataFrame.isnull().any()`

+ 使用apply：

  + 对name列进行大写转化：`df['name'] = df['name'].apply(str.upper)`

  + 定义函数进行处理：

    ```python
    def double_df(x):
        return 2*x
    df1[u'语文'] = df1[u'语文'].apply(double_df)  # 将语文成绩乘2处理（这里的u表示后面字符串以 Unicode 格式 进行编码，一般用在中文字符串前面，防止因为源码储存格式问题，导致再次使用时出现乱码）
    
    def plus(df,n,m):
        df['new1'] = (df[u'语文']+df[u'英语']) * m
        df['new2'] = (df[u'语文']+df[u'英语']) * n
        return df
    df1 = df1.apply(plus,axis=1,args=(2,3,))  # 新增两列，第一个参数是函数名，axis=1代表按照列为轴进行操作、axis=0代表行为轴进行操作，args传递参数，最终生成新的df
    ```

### 数据统计：

![pandas数据统计](D:\note_file\数据分析\pandas数据统计.jpg)

### 数据表合并(merge())：

+ 基于指定的列进行连接：`df3 = pd.merge(df1, df2, on='name')`——合并的是name列相同的索引
+ inner内连接：`df3 = pd.merge(df1, df2, how='inner')`——其实就是键的交集，合并的是所有相同的键的相同的索引
+ outer外连接：`df3 = pd.merge(df1, df2, how='outer')`——相当于求两个DataFrame的并集
+ left左连接：`df3 = pd.merge(df1, df2, how='left')`——以第一个 DataFrame 为主进行的连接，第二个 DataFrame 作为补充（必须有一列是相同的）
+ right右连接：`df3 = pd.merge(df1, df2, how='right')`——以第二个 DataFrame 为主进行的连接，第一个 DataFrame 作为补充（必须有一列是相同的）

### 使用SQL方式打开Pandas：

```python
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)
```

# 数据可视化

## 目的：

+ 分布、时间相关、局部/整体、偏差、相关性、排名、量级、地图、流动

## 常用工具：

+ 八爪鱼数据抓取、微图数据可视化
+ Tableau：商业数据可视化软件
+ DataV：数字大屏技术
+ Python语言可视化
+ R语言可视化

## Python数据可视化简述：

### 常用视图：

![常用视图](D:\note_file\数据分析\常用视图.jpg)

### 视图呈现的关系：

1. 比较：比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；
2. 联系：查看两个或两个以上变量之间的关系，比如散点图；
3. 构成：每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；
4. 分布：关注单个变量，或者多个变量的分布情况，比如直方图。

### 散点图：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
N = 1000
# numpy.random.randn(d0,d1,…,dn)是从标准正态分布中返回一个或多个样本值
x = np.random.randn(N)
y = np.random.randn(N)
# 用 Matplotlib 画散点图
plt.scatter(x, y, marker='x')
plt.show()
# 用 Seaborn 画散点图
df = pd.DataFrame({'x': x, 'y': y})
# sns.jointplot(x, y, data=None, kind=‘scatter’) 函数，data一般是DataFrame数据，前两个参数分别是DataFrame中的两个列的列名作为横纵坐标
sns.jointplot(x="x", y="y", data=df, kind='scatter');
plt.show()
```

### 折线图：

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]
# 使用 Matplotlib 画折线图
# 在 Matplotlib 中，我们可以直接使用 plt.plot() 函数，当然需要提前把数据按照 x 轴的大小进行排序，要不画出来的折线图就无法按照 x 轴递增的顺序展示
plt.plot(x, y)
plt.show()
# 使用 Seaborn 画折线图
# 在 Seaborn 中，我们使用 sns.lineplot (x, y, data=None) 函数。其中 x、y 是 data 中的下标。data 就是我们要传入的数据，一般是 DataFrame 类型
df = pd.DataFrame({'x': x, 'y': y})
sns.lineplot(x="x", y="y", data=df)
plt.show()
```

### 直方图：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
a = np.random.randn(100)
s = pd.Series(a) 
# 用 Matplotlib 画直方图
# 在 Matplotlib 中，我们使用 plt.hist(x, bins=10) 函数，其中参数 x 是一维数组，bins 代表直方图中的箱子数量，默认是 10
plt.hist(s)
plt.show()
# 用 Seaborn 画直方图
# 在 Seaborn 中，我们使用 sns.distplot(x, bins=10, kde=True) 函数。其中参数 x 是一维数组，bins 代表直方图中的箱子数量，kde 代表显示核密度估计
sns.distplot(s, kde=False)
plt.show()
sns.distplot(s, kde=True)
plt.show()
```

### 条形图：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = ['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5']
y = [5, 4, 8, 12, 7]
# 用 Matplotlib 画条形图
# 在 Matplotlib 中，我们使用 plt.bar(x, height) 函数，其中参数 x 代表 x 轴的位置序列，height 是 y 轴的数值序列，也就是柱子的高度
plt.bar(x, y)
plt.show()
# 用 Seaborn 画条形图
# 在 Seaborn 中，我们使用 sns.barplot(x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量
sns.barplot(x, y)
plt.show()
```

### 箱线图：

+ 中间那条横线是中位数，箱子的宽度是四分位数间距（IQR），也称为中间50%间距，是统计离散度的度量，越离散越长，等于第75和第25百分位数之间的差异，或者说是在上下四分位数之间，即：IQR = Q3-Q1。
+ 箱线图的下限就是Q1-1.5IQR，上限是Q3+1.5IQR。超过上限和下限的称为离散值。

```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
# 数据准备
# 生成 0-1 之间的 10*4 维度数据
data=np.random.normal(size=(10,4))
print(data)
lables = ['A','B','C','D']
# 用 Matplotlib 画箱线图
# 在 Matplotlib 中，我们使用 plt.boxplot(x, labels=None) 函数，其中参数 x 代表要绘制箱线图的数据，labels 是缺省值，可以为箱线图添加标签
plt.boxplot(data,labels=lables)
plt.show()
# 用 Seaborn 画箱线图
# 在 Seaborn 中，我们使用 sns.boxplot(x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量
df = pd.DataFrame(data, columns=lables)
print(df)
sns.boxplot(data=df)
plt.show()
```

### 饼图：

```python
import matplotlib.pyplot as plt
# 数据准备
nums = [25, 37, 33, 37, 6]
labels = ['High-school','Bachelor','Master','Ph.d', 'Others']
# 用 Matplotlib 画饼图
# 在 Matplotlib 中，我们使用 plt.pie(x, labels=None) 函数，其中参数 x 代表要绘制饼图的数据，labels 是缺省值，可以为饼图添加标签
plt.pie(x = nums, labels=labels)
plt.show()
```

### 热力图：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
flights = sns.load_dataset("flights")
data=flights.pivot('year','month','passengers')
# 用 Seaborn 画热力图
sns.heatmap(data)
plt.show()
```

### 蜘蛛图：

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.font_manager import FontProperties  
# 数据准备
labels=np.array([u" 推进 ","KDA",u" 生存 ",u" 团战 ",u" 发育 ",u" 输出 "])
stats=[83, 61, 95, 67, 76, 88]
# 画图数据准备，角度、状态值
angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
stats=np.concatenate((stats,[stats[0]]))
angles=np.concatenate((angles,[angles[0]]))
# 用 Matplotlib 画蜘蛛图
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)   
ax.plot(angles, stats, 'o-', linewidth=2)
ax.fill(angles, stats, alpha=0.25)
# 设置中文字体
font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf", size=14)  
ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)
plt.show()
```

### 二元变量分布：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
tips = sns.load_dataset("tips")
print(tips.head(10))
# 用 Seaborn 画二元变量分布图（散点图，核密度图，Hexbin 图）
# 在 Seaborn 里，使用二元变量分布是非常方便的，直接使用 sns.jointplot(x, y, data=None, kind) 函数即可。其中用 kind 表示不同的视图类型：“kind=‘scatter’”代表散点图，“kind=‘kde’”代表核密度图，“kind=‘hex’ ”代表 Hexbin 图，它代表的是直方图的二维模拟
sns.jointplot(x="total_bill", y="tip", data=tips, kind='scatter')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='kde')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='hex')
plt.show()
```

### 成对关系：

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
iris = sns.load_dataset('iris')
# 用 Seaborn 画成对关系
# 探索数据集中的多个成对双变量的分布，可以直接采用 sns.pairplot() 函数。它会同时展示出 DataFrame 中每对变量的关系，另外在对角线上，你能看到每个变量自身作为单变量的分布情况
sns.pairplot(iris)
plt.show()
```

# 数据分析中的基本概念

+ 数据挖掘核心：包括分类、聚类、预测、关联分析

## 元数据和数据源：

+ 元数据：描述其它数据的数据，也称为“中介数据”
+ 数据元：最小的数据单元

## 数据挖掘中的任务：

### 分类：

+ 通过训练集得到一个分类模型，然后用这个模型可以对其他数据（测试集）进行分类。

### 聚类：

+ 将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。

### 预测：

+ 通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。

### 关联分析：

+ 发现数据中的关联规则

## 数据挖掘的步骤：

1. 输入数据
2. 数据预处理
   1. 数据清洗：为了去除重复数据，去噪声以及填充缺失值
   2. 数据集成：将多个数据源中的数据存放在一个统一的数据存储中
   3. 数据变换：就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 0~1 之间。
3. 数据挖掘
4. 后处理：将模型预测的结果进一步处理后再导出
5. 信息

+ **上帝不会告诉我们规律，而是展示给我们数据**

## 数据清洗：

### 规则：

+ **完**整性：单条数据是否存在空值，统计的字段是否完善。

+ **全**面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。

+ **合**法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。

+ 唯**一**性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

## 数据集成：

### 两个架构：ETL和ELT

![image-20210810125019181](D:\note_file\数据分析\ETL和ELT.png)

### ETL的典型工具：Kettle

+ 两个脚本
  + Transformation（转换）：分成三个步骤，它包括了输入、中间转换以及输出。
    + Step（步骤）：Step 是转换的最小单元，每一个 Step 完成一个特定的功能。
    + Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。
  + Job（作业）：完整的任务，实际上是将创建好的转换和作业串联起来
    + Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务。
    + Hop（跳跃线）：连接 Job Entry 的线。并且它可以指定是否有条件地执行。

## 数据变换（比算法选择更重要）：

### 数据平滑：

+ 去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑

### 数据聚集：

+ 对数据进行汇总

### 数据概化：

+ 将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念

### 数据规范化：

+ 使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等

+ **最大—最小规范化**：将原始数据变换到 [0,1] 的空间中。用公式表示就是：`新数值 =（原数值 - 极小值）/（极大值 - 极小值）`

  ```python
  from sklearn import preprocessing
  import numpy as np
  # 初始化数据，每一行表示一个样本，每一列表示一个特征
  x = np.array([[ 0., -3.,  1.],
                [ 3.,  1.,  2.],
                [ 0.,  1., -1.]])
  # 将数据进行 [0,1] 规范化
  min_max_scaler = preprocessing.MinMaxScaler()
  minmax_x = min_max_scaler.fit_transform(x)
  print minmax_x
  ```

+ **Z-Score规范化**：`新数值 =（原数值 - 均值）/ 标准差`，结果易于比较但是没有实际意义

  ```python
  from sklearn import preprocessing
  import numpy as np
  # 初始化数据
  x = np.array([[ 0., -3.,  1.],
                [ 3.,  1.,  2.],
                [ 0.,  1., -1.]])
  # 将数据进行 Z-Score 规范化
  scaled_x = preprocessing.scale(x)
  # Z-Score 规范化将数据集进行了规范化，数值都符合均值为 0，方差为 1 的正态分布。
  print scaled_x
  ```

+ **小数定标规范化**：通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。`比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。`

  ```python
  from sklearn import preprocessing
  import numpy as np
  # 初始化数据
  x = np.array([[ 0., -3.,  1.],
                [ 3.,  1.,  2.],
                [ 0.,  1., -1.]])
  # 小数定标规范化
  j = np.ceil(np.log10(np.max(abs(x))))
  scaled_x = x/(10**j)
  print scaled_x
  ```

+ python工具：SciKit-Learn库

### 属性构造：

+ 构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程

# 算法：

## 决策树：

### 工作原理：

+ 决策树就是在当遇到更多选择的时候，我们是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，这个图实际上就是一个树状图，也就是决策树。
+ 分为构造和剪枝两个重要阶段。

### 构造：

+ 构造就是在生成一个决策树的时候，选择什么属性作为节点的过程。
+ 主要有三个节点：
  + 根节点
  + 内部节点
  + 叶节点

### 剪枝：

+ 目的是防止过拟合现象的发生
  + 过拟合现象：是说模型训练的太好了，这也会导致实际应用时分类错误，也就是模型的泛化能力差。
  + 造成过拟合的原因有训练集中样本量太小，或者决策树的属性过多，因为这样构造出来的决策树可以完美的分类训练集数据，因为把训练集中的数据的特点当成了所有数据的特点，这是不合理的。

+ 剪枝一般分为：
  + 预剪枝：在决策树构造时就进行剪枝。
    + 方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。
  + 后剪枝：生成决策树之后再进行剪枝。
    + 会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

### 信息熵和纯度：

+ 信息熵：表示信息的不确定度。如果一个信息熵越大，说明集合中所包含的信息量就越大，不确定性越大。
  + 在信息论中，随机离散事件出现的概率存在着不确定性。
  + ![image-20210904212109747](D:\note_file\数据分析\image-20210904212109747.png)
  + p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。

+ 纯度：和信息熵相反。信息熵越大，纯度越低。
+ 信息熵的计算跟属性没关系，只跟最终我要分类的结果有关。我们根据某一个属性进行分类，才会得到一个个子节点，对各个子节点计算信息熵并进行归一化，从而计算如此分类的信息增益。

### ID3算法：

+ 使用信息增益来决定节点，信息增益越大，越倾向于选择这个属性来作为节点

+ 信息增益的计算：公式表示的是父节点的信息熵减去所有子节点的归一化信息熵。

  ![image-20210904212815085](D:\note_file\数据分析\image-20210904212815085.png)

+ ![image-20210904214934001](D:\note_file\数据分析\image-20210904214934001.png)

### C4.5算法（ID3算法的改进）：

+ **采用信息增益率**：因为 ID3 在计算的时候，倾向于选择取值多的属性（因为这样能够分为更多的子节点，则每个子节点的信息熵更有可能小，这样信息增益就有可能更大）。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵。当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。 
+ **采用悲观剪枝**：ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。
+ **离散化处理连续属性**：C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。离散化处理的阈值是具有最高信息增益的划分。
+ **处理缺失值**：先忽略缺失值去计算信息增益，然后根据不缺失的样本所占比重来计算总的信息增益。

### CART算法（分类回归树）：

+ 既可以做分类树、也可以做回归树

  + 分类树：采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果。可使用sklearn中的DecisionTreeClassifier创建。

    ![image-20210905125935014](D:\note_file\数据分析\image-20210905125935014.png)

    ![image-20210905130015224](D:\note_file\数据分析\image-20210905130015224.png)

  + 回归树：采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的一句，得到的是连续值，即回归预测结果。可使用sklearn中的DecisionTreeRegressor创建。

+ 剪枝：

  + 主要采用CCP方法：是一种后剪枝的方法，也叫做代价复杂度。
    + 指标：表面误差率增益值，等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。
    + ![image-20210905131150909](D:\note_file\数据分析\image-20210905131150909.png)
    + 步骤：因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。

+ 例子（泰坦尼克乘客生存预测实战https://time.geekbang.org/column/article/79072）：

  ![f09fd3c8b1ce771624b803978f01c9ea](D:\note_file\数据分析\f09fd3c8b1ce771624b803978f01c9ea.jpg)

## 朴素贝叶斯：

### 定义：

+ 是在贝叶斯的基础上，假设每个输入变量（属性）是相互独立的。就是要分类的样本有很多个属性，要保证这些属性是相互独立的。

### 朴素贝叶斯的概率模型由两种类型的概率组成：

+ 每一个类别的概率P(Cj)
+ 每一个属性的条件概率P(Ai|Cj)

### 朴素贝叶斯分类器流程：

1. 准备阶段：对于数据，确定特征属性。分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。
2. 训练阶段：就是生成分类器，就是每个类别在训练样本中的出现频率P(Cj)，及每个特征属性对应每个类别的条件概率P(Ai|Cj)。
3. 应用阶段：使用分类器对新数据进行分类。即求新数据的每个特征属性具体取值下，对应的每一个分类的概率，然后把新数据分为概率大的那一类。
   + 对于离散型数据，已经计算好每个分类下这个离散属性取各种值的概率，应用时拿来用即可。
   + 对于连续型数据，无法直接计算属性取各个值的概率，一种方法是可以考虑使用训练数据的在每个分类下的样本均值和方差来构成正态分布，再根据新数据该连续性属性上所取的具体值，应用正态分布来计算每个分类下取这个值的概率，再根据贝叶斯的原理来计算新数据的属性取这个值时为各个分类的概率。

### 三种常用的朴素贝叶斯算法：

+ 高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。
+ 多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。
  + TF-IDF值：词频（Term Frequency）和逆向文档频（Inverse Document Frequency）的乘积。我们倾向于找到TF和IDF取值都高的单词作为区分。即一个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。
    + 词频 TF，计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。
    + 逆向文档频率 IDF，是指一个单词在文档中的区分度。它认为一个单词出现在的**文档数**越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。
  + ![image-20210907153846576](D:\note_file\数据分析\image-20210907153846576.png)
+ 伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。





































































